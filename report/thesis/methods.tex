\section{Causal analysis}
\label{sec:methods:causal}

\subsection{Potential Outcome Framework}

In order to credibly make the claim that the estimated effect is a causal one, some a priori considerations are in order.
To this end, the review by \citeA{imbens2024causal} is considered.

Firstly, there is the stable unit treatment value assumption (SUTVA), which is the assumption that the outcome for one
individual is not influenced by treatment exposure (in this case, engagement in exercise) of another.
Since the subjects of the LISS panel are selected to be representative of the Netherlands as a whole, the subjects
live all over the country and are not likely to know each other, let alone influence each other. However, it is typical
for multiple individuals to partake within a single household. On average, it is found that about 1.6 individuals participate
per household. Within one household, we cannot rule out that individuals influence each other. In fact, \citeA{maltby2012contextual}
find that one's perception of the benefit of exercise is influenced by social norms, clearly violating the SUTVA.
However, because these violations are local to small groups in the dataset, the assumption is taken to hold at large for
the studied data.

Next, we formalise the effect of interest, namely the causal effect of exercise on mental wellbeing, using the
potential outcome.
For each individual $i$, $Y_i(T)$ is the potential outcome if they engage in exercise (treatment),
and $Y_i(C)$ is the potential outcome if they don't (control). The variable of interest then is the sample average
treatment effect
\begin{equation}
\label{eq:methods:sate}
    \tau_{\text{sample}} = \sum_{i=1}^N (Y_i(T) - Y_i(C)),
\end{equation}
which thanks to the representativeness of the sample is assumed to estimate the population average treatement.
This is where SUTVA is crucial, as without it the potential outcomes are not simply a function of individual $i$'s treatment,
but also of individual $j$'s treatment (where $j \neq i$), and so we could not meaningfully interpret the average
treatment effect calculated as in \cref{eq:methods:sate}.
For any one individual the treatment effect may deviate from the average effect, in the sense that it may be moderated by
for instance personal circumstances or genetics, but such analysis is left to further research.

Denote $X_i$ as the treatment assignment for individual $i$, $X_i \in \{C, T\}$. The crucial assumption is that the
\textit{potential} outcomes are not influenced by the treatment assignment, or formally, given some set of controls
$W_i$,
\begin{equation}
    (Y_i(T), Y_i(C)) \perp\!\!\!\perp X_i \mid W_i.
\end{equation}
This assumption is called the ignorability assumption or the assumption of no unobserved confounders.
The controls are included to lend credibility to the assumption, as they remove biases in comparing treated individuals
to individuals in the control group.
Selection bias, MNAR data and the assumption's namesake in no unobserved confounders can all equivalently be considered
violations of the ignorability assumption.
The crucial difficulty that necessitates the assumption is that for any one individual $i$,
only either $Y_i(T)$ or $Y_i(C)$ is observed, and thus we cannot directly observe the difference in potential outcomes.
However, under the ignorability assumption, this difference can be estimated by $Y_i(T) - Y_j(C)$ for $j \neq i$,
which is a quantity that can be observed directly.

Considering the research question at hand is whether physical exercise is an effective intervention, the aim is not to
quantify the exact effect, rather to find whether a significant effect exists. To that end, the ignorability assumption
can be relaxed slightly to assuming that in so far as there are unmeasured confounders, they do not change the statistical
significance (nor the sign) of the estimated effect.

\subsection{Reverse Causality}
Now consider the aim of the present study, namely estimating the effect of exercise on mental health, $Y_i(T) - Y_i(C)$.
In a cross-sectional dataset, this effect is not identified,
because if mental health influences the probability to engage in exercise, the ignorability assumption is violated.
If for instance better mental health makes one more likely to engage in exercise, and assuming the greater mental health
does not disappear as soon as one engages in exercise, then the \textit{potential} outcomes $Y_i$ are increased
for individuals with better mental health. That is, in the observed sample, $Y_i(T) - Y_j(C)$ will be inflated with respect to
the average treatment effect.
This holds true even if the treatment effect itself is not influenced by a priori mental health.
\Cref{fig:methods:reverse_causality:cs} gives a graphical representation of how the forward and reverse effect
are not simultaneously identified.

On the other hand, with panel data, we can exploit the fact that cause must preceed effect. The graphical representation
in \cref{fig:methods:reverse_causality:panel} shows the instantaneous effects, along with presumed
autoregressive effects and a lagged effect.
The latter is now identified. Crucially, the instantaneous effect of sports on MHI5 is not identified, nor is the effect
of lagged sports through the autoregressive behaviour of MHI5.
Recalling that the established mechanisms through which sports may improve mental health can be split into short-term
and long-term mechanisms, it can thus be concluded that observational data provides no way to quantify the short-term effects,
at least not for any effects that last much shorter than the time difference between two waves of the panel.
The elevated endorphin levels post-exercise for instance would likely last on the order of hours (though there is no
research on the exact duration), which is clearly not identifiable on the yearly time-scale of the LISS panel.
Considering the various mechanisms for a positive short-term effect, and assuming a positive long-term effect is found,
it is reasonable to postulate that the short-term effect would also be positive.
Assuming positive autoregressive coefficients, both the aforementioned unidentified effects will be positive.
To the end of answering the research question then, we may conclude that the total benefit of exercise is at least
as large as the found long-term benefit. Thus, while the unidentified effects mean decreased power in answering
the research question, but significance in the sense of the type I error rate is preserved.

\begin{figure}[htbp]
    \caption{Graph representations of the interactions between MHI5-score and sports. Controls are left out for
    simplicity. Red arrows indicate unidentified effects}
    \label{fig:methods:reverse_causality}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[
            node distance=0.8cm,
            main node/.style={circle,draw,font=\scriptsize\sffamily\bfseries,minimum size=1.9cm},
            every edge/.style={draw,thick,->},
        ]
            \node[main node] (mhi5) {MHI5};
            \node[main node, below=of mhi5] (sports) {SPORTS};

            \path[bend left,color=red] (mhi5) edge (sports);
            \path[bend left,color=red] (sports) edge (mhi5);
        \end{tikzpicture}
        \subcaption{Cross-sectional data}
        \label{fig:methods:reverse_causality:cs}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[
            node distance=0.8cm,
            main node/.style={circle,draw,font=\scriptsize\sffamily\bfseries,minimum size=1.9cm},
            every edge/.style={draw,thick,->},
            fading line/.style={draw, thick, dotted, dash pattern=on 2pt off 2pt},
        ]
            \node[main node] (mhi5_t) {MHI5$_t$};
            \node[main node, below=of mhi5_t] (sports_t) {SPORTS$_t$};
            \node[main node, left=1.5cm of mhi5_t] (mhi5_t1) {MHI5$_{t-1}$};
            \node[main node, below=of mhi5_t1] (sports_t1) {SPORTS$_{t-1}$};

            \path[bend left,color=red] (mhi5_t) edge (sports_t);
            \path[bend left,color=red] (sports_t) edge (mhi5_t);
            \path[bend left,color=red] (mhi5_t1) edge (sports_t1);
            \path[bend left,color=red] (sports_t1) edge (mhi5_t1);
            \path (mhi5_t1) edge (mhi5_t);
            \path (sports_t1) edge (sports_t);
            \path[opacity=0.5] (mhi5_t1) edge (sports_t);
            \path[color=green!70!black] (sports_t1) edge (mhi5_t);

            \draw[fading line] ([xshift=-0.6cm]mhi5_t1.west) -- (mhi5_t1.west);
            \draw[fading line] ([xshift=-0.6cm]sports_t1.west) -- (sports_t1.west);
            \draw[fading line] (mhi5_t.east) -- ([xshift=0.6cm]mhi5_t.east);
            \draw[fading line] (sports_t.east) -- ([xshift=0.6cm]sports_t.east);
        \end{tikzpicture}
        \subcaption{Panel data. Note the effect of MHI5$_{t-1}$ on SPORTS$_t$ is identified,
        but is considered a nuisance parameter}
        \label{fig:methods:reverse_causality:panel}
    \end{subfigure}
\end{figure}

\section{Structural Equation Modelling}
\label{sec:methods:sem}
SEM can be considered a generalisation of the linear model, where rather than one regression equation which explains
the variability of the regressand with regards to a set of regressors, multiple regression equations are estimated simultaneously
to explain the total covariance structure of all variables involved.
The following will be a minimal introduction for the purpose of this work, in which accuracy is sacrificed to some degree
in favour of simplicity.
For a full introduction, consider textbooks such as \citeA{kline2023principles}.
While SEM also allows for modelling latent variables in its general form, such constructs are not considered
in this work. This is because the author possesses no a priori knowledge as to what latent constructs might exist, and
\citeA{ludtke2022comparison} find in a simulation study that empirically deciding on latent structures ``is only
of limited usefulness, beacuse the different model[l]ing approaches provide almost equivalent representations of [the data].''

Consider then a vector $z_i$, which contains for individual $i$ the outcome variable of interest $y_i$ as well as
all regressors $x$.
For the purpose of maximum likelihood estimation of the population parameters, note the well-known
likelihood (density) of a single observation
\begin{equation}
    f(z_i; \mu, \Sigma) = (2\pi)^{-\frac{p}{2}} |\Sigma|^{-\frac{1}{2}} e^{-\frac{1}{2}(z_i - \mu)' \Sigma^{-1}(z_i - \mu)},
\end{equation}
where $p$ denotes the dimension of $z_i$.
Assuming independent observations and dropping the constant, the sample log likelihood is
\begin{equation}
    \label{eq:methods:sample_likelihood}
    l(z) = \prod_{i=1}^N f(z_i; \mu, \Sigma)
    = -\frac{N}{2} \log(|\Sigma|) -\frac{1}{2} \sum_{i=1}^N (z_i - \mu)' \Sigma^{-1} (z_i - \mu),
\end{equation}
which for the purposes of optimisation can be rewritten to an equivalent fit function $F$, defined in terms of
the sample mean $\bar{x}$ and covariance matrix $S$ \cite{preacher2016ml} as
\begin{equation}
    \label{eq:methods:fit_function}
    F(\mu, \Sigma) = \log|\Sigma| + \text{tr}(S \Sigma^{-1}) - \log|S| - p + (\bar{z} - \mu)' \Sigma^{-1} (\bar{x} - \mu),
\end{equation}
with tr($\cdot$) denoting the trace of a matrix and the minimum of $F$ coincides with the maximum likelihood estimate.
Note this fit function does not directly involve any observation $z_i$. Rather it is defined purely in terms of the
sample moments $\bar{z}$ and $S$, which is to say that while \cref{eq:methods:sample_likelihood} and \cref{eq:methods:fit_function}
are theoretically equivalent, the latter operates on a higher level of abstraction.

At this stage, the elements of $\mu$ and $\Sigma$ may be estimated directly through numerical optimisation.
Firstly, note that $\mu$ can always be estimated as $\hat{\mu} = \bar{z}$, such that the last term in
\cref{eq:methods:fit_function} vanishes. It can thus be estimated separately from $\Sigma$,
and because the research question pertains specifically to the variability of variables,
the mean term is ignored in further discussion.
To see how this general multivariate likelihood approach relates to the linear model, consider the very simple example
regresson of
\begin{equation}
    \label{eq:methods:simple_regression}
    y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \epsilon.
\end{equation}
From this equation, the model-implied covariances between $y$
and $x_1$ and $x_2$ can be readily defined in terms of the regressor covariances as
\begin{align}
\begin{split}
    \label{eq:methods:covariance_simple_regression}
    \Sigma_{y x_1} &= \beta_1 \Sigma_{x_1} + \beta_2 \Sigma_{x_1 x_2}\text{ and} \\
    \Sigma_{y x_2} &= \beta_1 \Sigma_{x_1 x_2} + \beta_2 \Sigma_{x_2},
\end{split}
\end{align}
where $\Sigma_{x_1}$ ($\Sigma_{x_2}$) denotes the variance of $x_1$ ($x_2$) and $\Sigma_{x_1 x_2}$ denotes the covariance between $x_1$ and $x_2$.
Estimating $\Sigma_{y x_1}$ and $\Sigma_{y x_2}$ thus corresponds directly to estimating $\beta_1$ and $\beta_2$.
In general, the upper-right row of elements of the covariance matrix of $z$ is $\Sigma_{xy} = \Sigma_x \beta$ for
a vector of regressors $x$ and a vector of parameters $\beta$, where $\Sigma_x$ is the covariance matrix of $x$.
For completeness, the diagonal elements of $\Sigma$ are estimated directly as well as the covariance elements between
the regressors, at which point estimating the above regression equation is just a matter of reparametrisation as compared
to estimating the corresponding elements of $\Sigma$ directly; the two approaches are completely equivalent.

A benefit of the higher-level abstraction in SEM as compared the canonical linear model, is that multiple regressions
can be estimated simultaneously. Consider for instance
\begin{align}
\begin{split}
    \label{eq:methods:simultaneous_regression}
    y_1 &= \alpha_1 + \gamma y_2 + \epsilon_1;\\
    y_2 &= \alpha_2 + \beta x + \epsilon_2.
\end{split}
\end{align}
Note the subscripts in $y_1$ and $y_2$ denote different variables altogether,
not necessarily the same variable at different points in time.
Now, the model-implied $\Sigma_{y_1 x} = \gamma \beta \Sigma_x$.
However, because in general the second equation in \cref{eq:methods:simultaneous_regression} does not fully explain the
variance in $y_2$ ($R^2 < 1$), this may not fully explain the covariance between $y_1$ and $x$.
That is, there may be a residual covariance $\phi_{y_1 x}$, such that $\Sigma_{y_1 x} = \gamma \beta \Sigma_x + \phi_{y_1 x}$.
Note also that with three total variables, there are three covariance elements in $\Sigma$, but $\{\gamma, \beta\}$
constitutes only two parameters to estimate, which is too few parameters to exactly estimate the full covariance structure.

A general implication of the higher level of abstraction is that while canonically, the linear model has $N - k$ degrees
of freedom if there are $k$ parameters to be estimated, in SEM there are $\frac{p(p+1)}{2} - k$ degrees of freedom
for a model that involves $p$ variables. Note $\frac{p(p+1)}{2}$ is the number of unique elements in the covariance matrix.
The model in \cref{eq:methods:simple_regression} involves three variables (again, neglecting the mean components),
so again three covariance elements in $\Sigma$. Two of these are implied as in \cref{eq:methods:covariance_simple_regression},
and because both $x_1$ and $x_2$ only show up as regressors, the model does not imply any structure to their covariance,
and it is simply estimated as $\Sigma_{x_1 x_2} = \phi_{x_1 x_2}$. The residual variance of $y$ is also estimated along
with the two variances of the regressors, which leaves $\frac{p(p + 1)}{2} - k = 6 - 6 = 0$ degrees of freedom. This is
referred to as a model that is just identified.
The model in \cref{eq:methods:simultaneous_regression} differs in that it implies all three covariances, but with only two
parameters. If the model is not supplemented with $\phi_{y_1 x}$, there is $6 - 5 = 1$ degree of freedom, which is to
say that the model is overidentified.
While the example is trivial, in a more complex model overidentification may be exploited to quantify
whether the proposed model adequately describes the data. In other words, fit indices can be derived from these degrees
of freedom, in some sense analogous to the Sargan-Hansen test for the generalised method of moments.

SEM lends itself well to the present research, because it necessitates very few restrictions on the model.
This makes it a natural choice for exploring a potentially complex and unknown relationship between variables.
A panel regression fits into the SEM framework by having one regression for each wave of the panel,
i.e. $y_{i,1} = \ldots; y_{i, 2} = \ldots$, and so on. The framework then for instance allows adding autoregressive terms,
having (co)variances vary across waves or adding individual-specific effects. Fundamentally, the only restriction
imposed by SEM is that the covariance matrix is sufficient to identify the parameters.
Notably, while Arellano-Bond estimation of panel models relies on first-differencing the model for identification,
SEM does not and as such it is possible to include time-invariant controls beyond just the catch-all individual-specific effect.
